# Chapter 1：Wikidata 与对话数据：为什么它适合做“多样化主题”

## 1. 开篇段落：数据合成的“多样性瓶颈”

在当今的大语言模型（LLM）开发与微调（SFT）中，我们面临一个反直觉的现象：**获取海量数据很容易，但获取“不同”的数据很难。**

如果你试图用 LLM 自我博弈（Self-play）或简单爬取网页来生成对话数据，你会很快撞上**“多样性瓶颈”**：
1.  **模式坍塌（Mode Collapse）**：模型倾向于重复它训练数据中见过的高频模式。让你生成“100 个名人介绍”，它大概率会围着“爱因斯坦、埃隆·马斯克、迈克尔·杰克逊”转，而很难主动生成“18 世纪的日本俳句诗人”或“坦桑尼亚的地理学家”。
2.  **幻觉难以验证**：让模型“编造”一个复杂的逻辑推理对话，如果它编错了事实（例如搞错了某药品的化学成分），你很难用自动化手段检测出来。
3.  **长尾知识遗忘**：爬虫数据严重偏向头部流量内容（新闻、热点），导致模型在长尾领域（特定学科、冷门历史、非英语文化圈）表现不佳。

本章将介绍如何利用 **Wikidata（维基数据）** 作为破局的关键。我们将它不视为简单的“查词典”工具，而是将其视为一个**“可编程的主题采样器”**。通过它，我们可以构建出一套自动化流水线，生成覆盖面广、事实可追溯、难度可控的高质量中文对话数据。

**学习目标**：
1.  深入理解**结构化知识（KG）**与**非结构化文本（Corpus）**在合成数据中的本质差异。
2.  掌握**“可控采样（Controllable Sampling）”**的方法论：如何利用属性组合定义数据分布。
3.  理解**“可追溯性（Traceability）”**对数据清洗、修正和 LLM 幻觉抑制的工程价值。
4.  建立基于 Wikidata 的对话生成流水线（Pipeline）的全局视野。

---

## 2. 核心论述

### 2.1 为什么要用知识图谱造文本？

通常，我们获取知识有两种形态。理解它们的区别是设计数据流水线的前提。

*   **形态 A：非结构化文本 (Wikipedia/CommonCrawl)**
    *   **本质**：自然语言序列。
    *   **优点**：包含丰富的语境、修辞和逻辑连接。
    *   **缺点**：**不可查询**。你无法编写一行代码来说：“给我找出 50 个‘出生在二战期间’且‘配偶也是名人’的‘欧洲科学家’”。你只能靠搜索关键词，然后人工筛选。
*   **形态 B：结构化数据 (Wikidata)**
    *   **本质**：实体（Item）与属性（Property）构成的图（Graph）。
    *   **优点**：**完全可编程**。上述复杂的筛选条件，只是一条 SPARQL 查询语句。
    *   **缺点**：缺乏自然语言的润色（它只有“骨架”，没有“血肉”）。

**我们的策略**：**利用 Wikidata 的“可查询性”来解决采样问题，利用 LLM 的“生成能力”来解决润色问题。**

#### ASCII 对比：从“搜索”到“采样”

```text
[ 场景：我们需要构造 1000 条关于“建筑物历史”的问答数据 ]

❌ 传统爬虫/搜索方式 (Unstructured)
+-------------------------------------------------------------+
| 输入关键词: "著名建筑", "古老建筑", "摩天大楼"              |
| 结果:                                                       |
| 1. 故宫 (出现 500 次) - 严重重复                            |
| 2. 埃菲尔铁塔 (出现 300 次) - 头部效应                      |
| 3. 哈利法塔 (出现 100 次)                                   |
| ...                                                         |
| 结果特点:                                                   |
| - 很难找到"建于16世纪的非洲清真寺" (长尾丢失)               |
| - 无法轻易分离"高度"、"建筑师"等具体字段                    |
+-------------------------------------------------------------+

✅ Wikidata 结构化采样方式 (Structured)
+-------------------------------------------------------------+
| 编写 SPARQL 查询逻辑:                                       |
| Bucket A (20%): 摩天大楼 (P31=Q11303) AND 高度>300m         |
| Bucket B (30%): 宗教建筑 (P31=Q16970) AND 洲!=欧洲/北美     |
| Bucket C (50%): 桥梁/大坝 (P31=Q12280/Q12323) AND 年代<1900 |
|                                                             |
| 结果:                                                       |
| - Q123 (某知名大楼)                                         |
| - Q456 (某冷门印度神庙)                                     |
| - Q789 (某古罗马引水桥)                                     |
|                                                             |
| 结果特点:                                                   |
| - 强制均衡分布 (Forced Distribution)                        |
| - 获取纯净元数据: ID, 名称, 建造年份, 坐标                  |
+-------------------------------------------------------------+
```

### 2.2 解决“长尾”与“偏见”的系统工程

大模型训练数据的分布往往是偏斜的（Skewed）。主要体现在：
1.  **地域偏见**：英语世界内容过多，非英语世界（亚非拉）内容过少。
2.  **流行度偏见**：流量明星内容过多，专业领域/冷门人物内容过少。

Wikidata 提供了一个完美的修正机制。我们可以利用 **Sitelinks（站点链接数，即该词条被多少种语言的维基百科收录）** 作为“流行度”的代理指标。

**多样化采样算法 (Rule-of-Thumb):**

*   **头部数据 (Head)**: 采样 `Sitelinks > 20` 的实体。用于保证模型不傻，知道基本常识。
*   **腰部数据 (Torso)**: 采样 `5 < Sitelinks <= 20` 的实体。这是提升模型泛化能力的主战场。
*   **长尾数据 (Tail)**: 采样 `Sitelinks <= 5` 甚至 `Sitelinks = 1` 的实体。用于测试模型的 few-shot 学习能力或防止过拟合。

通过控制 SPARQL 查询中的 `FILTER` 条件，我们可以精准地控制数据集中包含 **30% 的欧美数据** 和 **70% 的亚非拉数据**，从而人为地对抗训练数据中的地域偏见。

### 2.3 可追溯性 (Traceability) 与数据血缘 (Lineage)

在工业级数据生产中，**“这行数据哪来的？”** 是一个灵魂拷问。

如果你的数据是纯 LLM 生成的：
> *Q: 为什么这条数据说李白会开飞机？*
> *A: 不知道，可能是模型抽风了，没法查。*

如果你的数据是基于 Wikidata 生成的：
> *Q: 为什么这条数据说李白属于唐朝？*
> *A: 因为它源自 Wikidata 实体 Q7071，属性 P2348 (period) 指向 Q318635 (Tang Dynasty)。这是数据生成的**证据链 (Evidence Chain)**。*

这种**结构化锚点 (Structured Anchor)** 带来了巨大的工程优势：
1.  **自动去重**：通过比较 QID 集合，可以确保训练集和测试集没有数据泄露（Data Leakage）。
2.  **版本更新**：如果 Wikidata 修正了某个事实（例如某人去世了），我们可以通过 ID 快速定位并更新相关的所有对话数据，而无需人工重读文本。
3.  **幻觉抑制**：在生成对话时，我们可以把从 Wikidata 查到的三元组放入 Prompt 作为 Context，强制模型“基于事实说话”。

### 2.4 数据生成全景流水线

这是本教程致力构建的最终系统架构。请仔细观察数据流向：

```ascii
Step 1: 策略层 (Strategy)
+--------------------------------------------------+
| 定义主题 Schema (YAML/JSON)                      |
| - 领域: [人物, 地理, 科学, 艺术...]              |
| - 难度分布: [简单:50%, 复杂推理:30%, 陷阱:20%]   |
+--------------------------------------------------+
        |
        v
Step 2: 检索层 (Retrieval - SPARQL)
+--------------------------------------------------+
| WDQS (Wikidata Query Service)                    |
| -> 执行查询 (Query Execution)                    |
| -> 过滤敏感词/无中文条目 (Filtering)             |
| -> 结果: JSON List [{"qid": "Q1", "fact":...}]   |
+--------------------------------------------------+
        |
        v
Step 3: 增强层 (Augmentation)
+--------------------------------------------------+
| 原始三元组 -> 自然语言化 (Linearization)         |
| - 模板法: "{S} 的 {P} 是 {O}"                    |
| - 补充上下文: 调用 Wikipedia 摘要 API 补全背景   |
+--------------------------------------------------+
        |
        v
Step 4: 生成层 (Synthesis - LLM)
+--------------------------------------------------+
| Input: 事实列表 + 对话风格 Prompt                |
| Model: "请基于上述事实，生成一段用户询问         |
|         关于该建筑历史的对话，包含一次追问。"    |
| Output: 多轮对话文本                             |
+--------------------------------------------------+
        |
        v
Step 5: 验证层 (Validation)
+--------------------------------------------------+
| - 事实一致性检查 (Fact Consistency Check)        |
| - 格式校验 (Schema Validation)                   |
| - 最终产出: dataset.jsonl (带 QID 元数据)        |
+--------------------------------------------------+
```

---

## 3. 本章小结

*   **核心痛点**：缺乏多样性和可控性是传统造数方法的瓶颈。
*   **Wikidata 的角色**：它不是百科全书的替代品，而是**结构化的索引**。它允许我们通过逻辑（SPARQL）而非概率（Prompt）来筛选数据源。
*   **采样哲学**：好数据 = 头部常识 + 广阔的长尾 + 均衡的地域/领域分布。Wikidata 是实现这一公式的最佳工具。
*   **工程价值**：通过保留 **QID** 和 **PID**，我们将非结构化的对话数据变成了可管理、可追踪、可更新的资产。

---

## 4. 练习题

### 基础题 (Basic)

1.  **数据形态辨析**：
    假设我们要造一批关于“诺贝尔奖得主”的数据。
    (A) 写爬虫抓取诺贝尔奖官网的获奖者列表网页文本。
    (B) 查询 Wikidata 中 `P166 (获奖)` 属性值为 `Q7191 (诺贝尔奖)` 的实体。
    请列出方法 (B) 相比方法 (A) 在**数据清洗**阶段的两个具体优势。
    *Hint*: 考虑一下名字的拼写变体和日期的格式。

    <details>
    <summary>点击查看参考答案</summary>

    1.  **消歧与标准化**：方法 A 得到的可能是 "Marie Curie", "M. Curie", "居里夫人" 等各种文本写法，需要复杂的清洗归一化。方法 B 直接得到唯一 ID `Q7186`，天然去重且标准化。
    2.  **结构化字段分离**：方法 A 需要从文本中用正则或 NLP 提取获奖年份、学科等信息，容易出错。方法 B 中，年份通常直接作为限定符（P585 point in time）以标准日期格式存储，学科也是独立的 QID，无需清洗自然语言。
    </details>

2.  **ID 认知**：
    在 Wikidata 中，`Q42` 代表道格拉斯·亚当斯，`P31` 代表“是...的实例”。如果我们发现一条声明是 `Q42 P31 Q5`，且 `Q5` 的标签是“人类”。这句话翻译成中文事实是什么？
    *Hint*: 这是一个最基础的三元组 (Subject, Predicate, Object)。

    <details>
    <summary>点击查看参考答案</summary>

    **事实**：道格拉斯·亚当斯是人类。
    (Subject: Douglas Adams, Predicate: instance of, Object: human)
    </details>

3.  **Sitelinks 理解**：
    如果一个实体的 `sitelinks` 计数为 150，另一个实体的 `sitelinks` 计数为 1。这通常分别意味着什么？在构造“高难度”问答题时，你会优先选哪一个？
    *Hint*: Sitelink 代表该实体在多少个不同语言的维基百科中有条目。

    <details>
    <summary>点击查看参考答案</summary>

    - **150 sitelinks**: 代表这是一个**全球知名**的实体（如“巴黎”、“迈克尔·乔丹”），几乎所有语言的维基百科都收录了它。
    - **1 sitelink**: 代表这是一个**极度冷门或本地化**的实体（如某个偏僻乡村的小学、某个不知名的地方官员），可能只在其母语维基百科有记载。
    - **构造“高难度”题目**: 优先选 **sitelinks = 1** 的实体。因为模型在预训练阶段极少见到相关语料，更能测试模型的知识边界或检索增强（RAG）能力。
    </details>

4.  **许可协议与合规**：
    某初创公司抓取了 Wikidata 的数据，训练了一个用于付费 API 的垂直领域模型，并且没有公开模型权重。这样做是否侵犯了 Wikidata 的版权？
    *Hint*: 关键词 CC0。

    <details>
    <summary>点击查看参考答案</summary>

    **不侵权。**
    Wikidata 使用 **CC0 (Public Domain)** 协议。这意味着数据没有任何版权限制。商业公司可以任意使用、修改、分发，甚至用于闭源商业项目，而无需支付费用或强制开源衍生成果。这是 Wikidata 相比 Wikipedia (CC-BY-SA) 最大的商业优势之一。
    </details>

### 挑战题 (Challenge)

5.  **采样策略设计**：
    你需要为一个针对“儿童科普”的对话机器人生成数据。儿童只能理解常见概念。如果你直接从 Wikidata 随机采样“动物”，可能会采到深海多毛蠕虫（Polychaeta）。
    请设计 2-3 条**过滤规则**，确保采样的实体适合儿童认知。
    *Hint*: 什么指标能反映一个概念是否“通俗易懂”？

    <details>
    <summary>点击查看参考答案</summary>

    **策略建议：**
    1.  **利用 Sitelinks 进行过滤**：设定阈值，例如 `FILTER(?sitelinks > 30)`。只有在 30 种以上语言都有词条的动物（如狮子、老虎、企鹅）才更有可能是常见动物。
    2.  **利用“通用性”属性**：检查该动物是否有对应的 Emoji 字符（P487 Unicode character），或者是否有由常见绘本/动画片引用的属性（虽然这很难直接查，但思路是找“文化关联”）。
    3.  **利用外部列表**：先找到“Q200569 (常见家畜)”或“Qxxx (动物园常见动物)”这类集合类目，然后只查询其子类。
    </details>

6.  **反事实与去重思考**：
    假设你在构建一个“多轮推理”数据集。你需要生成这样的对话：“A的妻子是B，B的丈夫是A吗？”
    在 Wikidata 中，这种关系（P26 配偶）通常是双向存储的（A 有 P26 指向 B，B 也有 P26 指向 A）。
    如果在采样时处理不当，会导致什么样的数据冗余问题？如何解决？
    *Hint*: 这里的冗余是指“逻辑上的重复”，即 (A, B) 和 (B, A) 是同一件事。

    <details>
    <summary>点击查看参考答案</summary>

    **问题**：
    如果在遍历所有实体的 P26 属性时，不加控制，你会被生成两条逻辑完全重复的数据：
    1. “张三的妻子是谁？是李四。”
    2. “李四的丈夫是谁？是张三。”
    这会导致模型过拟合这种简单的互逆关系，且浪费 Token。

    **解决方案**：
    在 SPARQL 查询或后处理中，利用 ID 的字典序进行去重。
    `FILTER(STR(?personA) < STR(?personB))`
    这行代码强制只取 ID 较小的那个作为“主体”，从而保证每一对配偶关系 `(A, B)` 只会被采样一次，无论它们在数据库中存储了多少次。
    </details>

7.  **开放性场景**：
    Wikidata 的数据虽然结构化，但并非所有数据都适合“对话”。请举出一个**不适合**直接转化为口语对话的 Wikidata 数据类型，并说明原因。
    *Hint*: 想象一下把一串枯燥的代码或标识符念出来的感觉。

    <details>
    <summary>点击查看参考答案</summary>

    **不适合的类型示例：外部标识符 (External Identifiers)**。
    Wikidata 包含大量指向其他数据库的 ID，例如“IMDb 编号 (P345)”、“国会图书馆控制号 (P244)”、“DOI (P356)”。
    
    **原因**：
    1.  **不可读性**：对话中出现“这部电影的 IMDb 编号是 tt0111161”非常生硬，除非是特定查询场景。
    2.  **无信息量**：对于普通用户闲聊，一串 ID 没有任何语义价值。
    
    **处理建议**：在构建对话数据时，应设立**属性黑名单**，明确排除此类标识符属性，只保留语义属性（时间、地点、人物关系、物理性质等）。
    </details>

---

## 5. 常见陷阱与错误 (Gotchas)

在后续章节开始实操前，请先建立对这些常见坑的认知：

1.  **"Real World" != "Wikidata World"**
    *   *现象*：你觉得“西红柿”肯定是“蔬菜”，但在 Wikidata 里，它可能被定义为“浆果 (berry)”或“果实”。
    *   *应对*：不要用你的常识去硬套查询逻辑。务必先在 WDQS 页面上手动查几个样例，看看它们的 `P31` (instance of) 到底填的是什么。

2.  **超时 (Timeout) 是家常便饭**
    *   *现象*：你写了一个完美的查询：“查找全人类的配偶”。点击运行，转圈 60 秒后报错 `Time out`。
    *   *应对*：Wikidata 查询服务有严格的计算资源限制。不要试图一次性拉取全量数据。必须学会**分页 (LIMIT/OFFSET)** 和 **分块采样 (按年份、按国家切分任务)**。这将在第 6 章详细讲解。

3.  **粒度混淆：P31 (是...的实例) vs P279 (是...的子类)**
    *   *现象*：你想找“所有程序员”，查询 `?x P31 程序员`。结果发现很少。因为大部分程序员的 P31 填的是“人类”，而职业属性 P106 填的是“程序员”。
    *   *或者*：你想找“所有犬科动物”，查 `?x P31 犬科`，结果为空。因为“犬科”是一个生物学分类，具体的狗（如史努比）是“狗”的实例，而“狗”是“犬科”的子类。
    *   *应对*：这将在第 2 章和第 3 章通过 `wdt:P31/wdt:P279*` 路径查询语法专门解决。

4.  **文本陷阱：Description 不等于 Article**
    *   *现象*：期望通过 Wikidata 获得实体的详细介绍。
    *   *现实*：Wikidata 的 `description` 字段通常只有短短几个词（例如：“1990年出生的日本歌手”）。如果你需要长文本来做阅读理解训练，必须使用 Wikidata 提供的 URL 去爬取对应的 Wikipedia 页面，而不是指望 Wikidata 本身提供大段文本。

---

[下一章：Chapter 2 - Wikidata 数据模型速通](chapter2.md)
